# Generate UAT Instructions for Linear Issue

Generate comprehensive UAT (User Acceptance Testing) instructions from a Linear issue, producing Given-When-Then acceptance criteria, numbered test steps, and a structured QA handoff document. Optionally post results to Linear.

## Parameters:

- **Linear Issue ID** (optional): The Linear issue identifier (e.g., `ONEK-178`)
- **`--all`** (optional): Process all in-progress issues for the team
- Usage: `/uat_instructions <linear-issue-id>` or `/uat_instructions --all`

## Actions to Execute:

### Step 1: Parse Arguments and Resolve Issues

1. **Parse the arguments** from `$ARGUMENTS`:
   - If a single issue ID is provided (e.g., `ONEK-178`):
     - Validate format matches `ONEK-\d+` pattern
     - Fetch the issue using the Linear MCP `get_issue` tool with `includeRelations: true`
     - If the issue is not found, report the error and stop
     - Check existing comments for a previous UAT instructions post (search for "## UAT Instructions" or "## Acceptance Criteria" in comment bodies using `list_comments`). If found, warn: "UAT instructions were previously posted on {date}. Continue to generate updated instructions? (yes/cancel)"
   - If `--all` is provided:
     - Fetch all in-progress issues using `list_issues` with `team: "One Kaleidoscope"`, `state: "started"`
     - Display a numbered list of issues found
     - Ask: "Generate UAT instructions for all {N} issues? (yes/select/cancel)"
     - If **select**: let user pick specific issues by number
     - If **cancel**: abort
     - Process each selected issue through Steps 2-6
   - If no arguments provided:
     - Ask the user: "Which Linear issue should I generate UAT instructions for? (e.g., ONEK-178, or --all for in-progress issues)"

### Step 2: Gather Context

1. **Fetch the issue details** — store title, description, status, assignee, labels, and priority

2. **Fetch parent epic** (if the issue has a parent):
   - Use the parent issue ID from the issue relations to get epic-level context

3. **Search git log for the issue ID**:
   ```bash
   git log --all --oneline --grep="$ISSUE_ID" | head -20
   ```
   - Collect commit messages referencing this issue

4. **Find related plan documents**:
   ```bash
   find docs/plans/ -name "*${ISSUE_ID}*" -o -name "*$(echo $ISSUE_TITLE | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | head -c 30)*" 2>/dev/null
   ```

5. **Identify affected files from git history**:
   ```bash
   git log --all --name-only --pretty=format: --grep="$ISSUE_ID" | sort -u | head -30
   ```
   - Group by area: components/, lib/, app/api/, mcp-servers/, agents/, __tests__/

6. **Check for existing test files**:
   ```bash
   find __tests__/ -name "*$(echo $ISSUE_TITLE | tr ' ' '-' | tr '[:upper:]' '[:lower:]' | head -c 20)*" 2>/dev/null
   ```

### Step 3: Extract or Generate Acceptance Criteria

Apply a tiered extraction strategy:

1. **Tier 1 — Parse existing AC from the issue description**:
   - Look for an "Acceptance Criteria" section header
   - Look for checkbox items (`- [ ]` or `- [x]`)
   - Look for "should", "must", "shall" statements
   - Look for numbered requirements lists
   - If found, convert each into Given-When-Then format

2. **Tier 2 — Synthesize from context** (if Tier 1 yields fewer than 2 criteria):
   - Derive criteria from the issue title and description
   - Use commit messages to understand what was implemented
   - Use affected files to understand scope (UI changes = visual criteria, API changes = functional criteria)
   - Create GWT scenarios that cover the primary feature behavior

3. **Tier 3 — Always include regression criteria**:
   - Add at least one regression criterion: "Existing {related feature} continues to work correctly"
   - If the issue is a bug fix, add a criterion that the original bug is resolved
   - If the issue touches shared components, add criteria for unaffected consumers

**Each criterion must include**:
- **Scenario**: Descriptive name (e.g., "User adjusts margin slider")
- **Given**: Precondition or initial state
- **When**: Action performed by the user or system
- **Then**: Expected observable result

**Minimum**: 3 criteria per issue (1 primary + 1 edge case or secondary + 1 regression)

### Step 4: Generate UAT Document

Build the UAT document using this template:

```markdown
# UAT Instructions: {ISSUE-ID}

**Issue**: [{ISSUE-ID}] {Issue Title}
**Date Generated**: {YYYY-MM-DD}
**Branch**: `{branch-name}` (from git log)
**Status**: {issue status}
**Priority**: {issue priority}
**Author**: Generated by Claude Code `/uat_instructions`

---

## Overview

{2-3 sentence summary of what was implemented or fixed, derived from the issue description and commit history}

---

## Acceptance Criteria

### AC-1: {Scenario Name}

**Given** {precondition}
**When** {action}
**Then** {expected result}

### AC-2: {Scenario Name}

**Given** {precondition}
**When** {action}
**Then** {expected result}

### AC-3: {Scenario Name} (Regression)

**Given** {precondition for existing functionality}
**When** {action that should still work}
**Then** {expected unchanged behavior}

---

## Test Steps

### Test 1: {Maps to AC-1 — Scenario Name}

1. {Navigate to / Open / Start at ...}
2. {Perform action ...}
3. {Observe ...}
   - **Expected**: {specific expected result}
4. {Verify ...}
   - **Expected**: {specific expected result}

**Result**: [ ] Pass  [ ] Fail

**Notes**: _______________

---

### Test 2: {Maps to AC-2 — Scenario Name}

1. {Step ...}
2. {Step ...}
   - **Expected**: {specific expected result}
3. {Step ...}
   - **Expected**: {specific expected result}

**Result**: [ ] Pass  [ ] Fail

**Notes**: _______________

---

### Test 3: {Maps to AC-3 — Regression}

1. {Step ...}
2. {Step ...}
   - **Expected**: {specific expected result}

**Result**: [ ] Pass  [ ] Fail

**Notes**: _______________

---

## Test Data & Environment

### Environment Requirements

- **URL**: {staging URL or localhost}
- **Branch**: `{branch-name}`
- **Setup**: `npm run dev` (starts app + MCP servers)

### Test Data

- {Specific test data needed, e.g., "Use Trip ID `trp-123456` with 3 operator quotes"}
- {Test user accounts or credentials needed}
- {Any feature flags or env vars required}

### Pre-Test Setup

1. {Any required setup steps}
2. {Clear cache / Reset state if needed}

---

## Sign-Off

| Tester | Role | Result | Date | Signature |
|--------|------|--------|------|-----------|
| @AB | QA Lead | [ ] Pass [ ] Fail | ____-__-__ | _________ |
| @Kham | Developer | [ ] Pass [ ] Fail | ____-__-__ | _________ |

### Final Disposition

- [ ] All tests passed — ready for production
- [ ] Issues found — see notes above
- [ ] Blocked — requires {description}

---

*Generated by `/uat_instructions {ISSUE-ID}` on {YYYY-MM-DD}*
```

### Step 5: Save to `docs/uat/`

1. **Generate the filename**:
   - Take the issue title, lowercase it, replace spaces and special characters with hyphens, truncate to 50 characters
   - Format: `UAT-{ISSUE-ID}-{slug}.md`
   - Example: `UAT-ONEK-206-tool-ui-registry.md`

2. **Ensure the directory exists**:
   ```bash
   mkdir -p docs/uat
   ```

3. **Write the file** to `docs/uat/{filename}`

4. **Report the saved path** to the user

### Step 6: Post to Linear (Optional)

1. **Ask the user** how to post:
   - **(1) Post as comment** (recommended) — Non-destructive, appears in issue activity
   - **(2) Append to description** — Adds AC checklist to the bottom of the issue description
   - **(3) Both** — Comment with full UAT + append AC checkboxes to description
   - **(4) Neither** — Skip Linear posting, file only

2. **If posting as comment**:
   - Compose a condensed version of the UAT document:
     ```markdown
     ## UAT Instructions

     **Generated**: {date} | **File**: `docs/uat/{filename}`

     ### Acceptance Criteria

     - [ ] **AC-1**: {Scenario} — Given {precondition}, when {action}, then {result}
     - [ ] **AC-2**: {Scenario} — Given {precondition}, when {action}, then {result}
     - [ ] **AC-3**: {Scenario (Regression)} — Given {precondition}, when {action}, then {result}

     ### Quick Test Steps

     1. **{Test 1}**: {1-line summary of key action and expected result}
     2. **{Test 2}**: {1-line summary of key action and expected result}
     3. **{Test 3}**: {1-line summary of key action and expected result}

     **Full instructions**: `docs/uat/{filename}`

     @AB @Kham — UAT instructions ready for review.
     ```
   - **Show the comment to the user** for review before posting
   - Post using `create_comment` with the issue ID

3. **If appending to description**:
   - Fetch current description using `get_issue`
   - Append a new section (never overwrite existing content):
     ```markdown

     ---

     ## Acceptance Criteria (UAT)

     - [ ] {AC-1 one-line summary}
     - [ ] {AC-2 one-line summary}
     - [ ] {AC-3 one-line summary}

     _Full UAT instructions: `docs/uat/{filename}`_
     ```
   - Update using `update_issue` with the appended description
   - **Show the appended section to the user** before posting

### Step 7: Summary Report

Report to the user:

**Single issue mode**:
```
UAT Instructions Generated

  Issue:      {ISSUE-ID} — {title}
  File:       docs/uat/{filename}
  Criteria:   {N} acceptance criteria (Given-When-Then)
  Tests:      {N} numbered test steps
  Linear:     {Comment posted / Description updated / Both / Skipped}

Suggested next steps:
  1. Execute the test steps manually or with a tester
  2. Run /e2e-test-issue {ISSUE-ID} for automated browser verification
  3. Run /linear-update-summary {ISSUE-ID} to document results
```

**Batch mode** (`--all`):
```
UAT Instructions Generated — Batch Summary

| Issue | Title | Criteria | Tests | File | Linear |
|-------|-------|----------|-------|------|--------|
| ONEK-XXX | {title} | {N} | {N} | docs/uat/{file} | {status} |
| ONEK-YYY | {title} | {N} | {N} | docs/uat/{file} | {status} |

Total: {N} issues processed, {total criteria} acceptance criteria, {total tests} test steps

Suggested next steps:
  1. Review generated files in docs/uat/
  2. Distribute to testers (@AB, @Kham)
  3. Run /e2e-test-issue for automated verification
```

## Notes:

- If the **Linear MCP is not available**, output the UAT document to the console and save the file locally — skip the Linear posting step
- **Duplicate detection**: If a UAT comment already exists on the issue, warn the user before posting another. Offer to update the existing comment or post a new version
- **Lifecycle integration**: This command fits between development and sign-off:
  ```
  /linear-fix-issue ONEK-XXX       (triage + fix)
  /e2e-test-issue ONEK-XXX         (browser verification)
  /uat_instructions ONEK-XXX       (generate UAT plan)   <-- THIS COMMAND
  /linear-update-summary ONEK-XXX  (document for handoff)
  ```
- **Minimum quality bar**: Every UAT document must have at least 3 acceptance criteria and 3 corresponding test steps. If the issue lacks sufficient context, warn the user and generate what's possible with a "needs review" flag
- The command works from any branch — it uses `git log --all` to find commits across all branches
- Test steps should be specific enough that a non-developer can follow them (include UI element names, expected text, navigation paths)
- The Sign-Off table always includes @AB and @Kham as default testers
